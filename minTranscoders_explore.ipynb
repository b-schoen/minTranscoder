{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8136c449-8445-45fd-8d23-358f6e639e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: same thing for REPL\n",
    "# note: we use this instead of magic because `black` will otherwise fail to format\n",
    "#\n",
    "# Enable autoreload to automatically reload modules when they change\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "# do this so that formatter not messed up\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Import commonly used libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# type annotation\n",
    "import jaxtyping\n",
    "from jaxtyping import Float, Float32, Int64, jaxtyped\n",
    "from typeguard import typechecked as typechecker\n",
    "\n",
    "# more itertools\n",
    "import more_itertools as mi\n",
    "\n",
    "# itertools\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "# tensor manipulation\n",
    "import einops\n",
    "\n",
    "# automatically apply jaxtyping\n",
    "# %load_ext jaxtyping\n",
    "# %jaxtyping.typechecker typeguard.typechecked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1271761",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57a2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "# load the device we'll use (GPU or MPS)\n",
    "device = transformer_lens.utils.get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fccea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory usage has been reset.\n",
      "Current CUDA memory allocated: 0.00 MB\n",
      "Current CUDA memory cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Reset CUDA memory usage\n",
    "import torch\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Optionally, you can also run garbage collection\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"CUDA memory usage has been reset.\")\n",
    "\n",
    "# Check current memory usage after reset\n",
    "if torch.cuda.is_available():\n",
    "    print(\n",
    "        f\"Current CUDA memory allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\"\n",
    "    )\n",
    "    print(f\"Current CUDA memory cached: {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3bb617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minTranscoder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model_name = \"gpt2-small\"\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1587a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.07</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m70.07\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.09 Prob: 70.07% Token: | Mary|\n",
      "Top 1th token. Logit: 15.38 Prob:  4.67% Token: | the|\n",
      "Top 2th token. Logit: 15.35 Prob:  4.54% Token: | John|\n",
      "Top 3th token. Logit: 15.25 Prob:  4.11% Token: | them|\n",
      "Top 4th token. Logit: 14.84 Prob:  2.73% Token: | his|\n",
      "Top 5th token. Logit: 14.06 Prob:  1.24% Token: | her|\n",
      "Top 6th token. Logit: 13.54 Prob:  0.74% Token: | a|\n",
      "Top 7th token. Logit: 13.52 Prob:  0.73% Token: | their|\n",
      "Top 8th token. Logit: 13.13 Prob:  0.49% Token: | Jesus|\n",
      "Top 9th token. Logit: 12.97 Prob:  0.42% Token: | him|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check with an example\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    model,\n",
    "    prepend_bos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54629d69",
   "metadata": {},
   "source": [
    "# Define Transcoder Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b3c8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "from min_transcoder.transcoder import (\n",
    "    TranscoderResults,\n",
    "    TranscoderConfig,\n",
    "    Transcoder,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TranscoderTrainingConfig:\n",
    "\n",
    "    # Name of the layer to hook into for feature extraction\n",
    "    hook_point: str\n",
    "    out_hook_point: str\n",
    "\n",
    "    num_epochs: int = 100\n",
    "\n",
    "    # from https://arxiv.org/html/2406.11944v1#S3 appendix E\n",
    "    learning_rate: float = 2 * 10e-5\n",
    "    l1_coefficient: float = 1e-4  # Coefficient for L1 regularization\n",
    "\n",
    "    @property\n",
    "    def hook_point_layer(self) -> int:\n",
    "        \"Parse out the hook point layer as int ex: 'blocks.8.ln2.hook_normalized' -> 8\"\n",
    "        return int(self.hook_point.split(\".\")[1])\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TranscoderLoss:\n",
    "    mse_loss: Float[torch.Tensor, \"\"]\n",
    "    l1_loss: Float[torch.Tensor, \"\"]\n",
    "\n",
    "\n",
    "def compute_loss(\n",
    "    cfg: TranscoderTrainingConfig,\n",
    "    mlp_out: Float[torch.Tensor, \"...\"],\n",
    "    results: TranscoderResults,\n",
    ") -> TranscoderLoss:\n",
    "\n",
    "    mse_loss_per_batch: Float[torch.Tensor, \"...\"] = (\n",
    "        torch.pow((results.transcoder_out - mlp_out.float()), 2)\n",
    "        / (mlp_out**2).sum(dim=-1, keepdim=True).sqrt()\n",
    "    )\n",
    "\n",
    "    mse_loss = mse_loss_per_batch.mean()\n",
    "\n",
    "    sparsity = torch.abs(results.hidden_activations).sum(dim=1).mean(dim=(0,))\n",
    "\n",
    "    # TODO(bschoen): Do we sum here?\n",
    "    l1_loss = cfg.l1_coefficient * sparsity.mean()\n",
    "\n",
    "    return TranscoderLoss(mse_loss=mse_loss, l1_loss=l1_loss)\n",
    "\n",
    "\n",
    "# from https://arxiv.org/html/2406.11944v1#S3 appendix E\n",
    "transcoder_expansion_factor = 32\n",
    "\n",
    "transcoder_cfg = TranscoderConfig(\n",
    "    d_in=model.cfg.d_model,\n",
    "    d_out=model.cfg.d_model,\n",
    "    # our transcoder has a hidden dimension of d_mlp * expansion factor\n",
    "    d_hidden=model.cfg.d_mlp * transcoder_expansion_factor,\n",
    "    dtype=model.cfg.dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab8b12ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.cfg.n_layers=12\n",
      "model.cfg.d_mlp=3072\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model.cfg.n_layers=}\")\n",
    "print(f\"{model.cfg.d_mlp=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1cbe0b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26733ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def create_tokenized_dataloader(\n",
    "    max_length: int = 128,\n",
    "    batch_size: int = 128,\n",
    "    num_samples: int = 10000,\n",
    ") -> DataLoader:\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = datasets.load_dataset(\n",
    "        path=\"NeelNanda/pile-10k\",\n",
    "        split=\"train\",\n",
    "        streaming=False,\n",
    "    )\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    token_dataset = transformer_lens.utils.tokenize_and_concatenate(\n",
    "        dataset=dataset,\n",
    "        tokenizer=model.tokenizer,\n",
    "        streaming=True,\n",
    "        max_length=max_length,\n",
    "        add_bos_token=model.cfg.default_prepend_bos,\n",
    "    )\n",
    "\n",
    "    # token_dataset['tokens'].shape=torch.Size([136625, 128])\n",
    "    # print(f\"{token_dataset['tokens'].shape=}\")\n",
    "\n",
    "    # shuffle, and arbitrarily cap at around 10,000 / 130,000 (original caps at ~24k)\n",
    "    token_dataset = token_dataset.shuffle(42)\n",
    "    token_dataset = token_dataset.take(num_samples)\n",
    "\n",
    "    token_dataset_torch = torch.from_numpy(\n",
    "        np.stack([x[\"tokens\"] for x in token_dataset])\n",
    "    ).cuda()\n",
    "\n",
    "    # torch.Size([100, 1024])\n",
    "    print(token_dataset_torch.shape)\n",
    "\n",
    "    # Create a DataLoader for batching\n",
    "    #\n",
    "    # for batch in dataloader:\n",
    "    #     print(batch.shape) # torch.Size([32, 1024])\n",
    "    #     break\n",
    "    #\n",
    "    print(f\"Creating dataloader for dataset...\")\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        token_dataset_torch,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Num batches: {token_dataset_torch.shape[0] / batch_size}\")\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd288a",
   "metadata": {},
   "source": [
    "# Collect Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f8c2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(bschoen): Learning rate scheduler\n",
    "\n",
    "\n",
    "# Define training parameters\n",
    "training_cfg = TranscoderTrainingConfig(\n",
    "    num_epochs=5,\n",
    "    hook_point=\"blocks.8.ln2.hook_normalized\",\n",
    "    out_hook_point=\"blocks.8.hook_mlp_out\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the MLP activations\n",
    "mlp_inputs: list[Float[torch.Tensor, \"batch seq d_mlp_in\"]] = []\n",
    "mlp_outputs: list[Float[torch.Tensor, \"batch seq d_mlp_out\"]] = []\n",
    "\n",
    "\n",
    "# TODO(bschoen): Could make this general\n",
    "def store_mlp_inputs(\n",
    "    mlp_input: Float[torch.Tensor, \"... d_in\"],\n",
    "    hook: transformer_lens.hook_points.HookPoint,\n",
    ") -> None:\n",
    "\n",
    "    # Detach and move to CPU to save memory\n",
    "    mlp_inputs.append(mlp_input.detach().cpu())\n",
    "\n",
    "\n",
    "def store_mlp_output(\n",
    "    mlp_output: Float[torch.Tensor, \"... d_out\"],\n",
    "    hook: transformer_lens.hook_points.HookPoint,\n",
    ") -> None:\n",
    "\n",
    "    # Detach and move to CPU to save memory\n",
    "    mlp_outputs.append(mlp_output.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eede8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_tokenized_dataloader()\n",
    "\n",
    "# put model itself into eval mode so doesn't change\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch in tqdm.tqdm(\n",
    "    enumerate(dataloader),\n",
    "    desc=\"Collecting MLP activations\",\n",
    "):\n",
    "    # move batch to device\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    # Get MLP input and output activations\n",
    "    model.run_with_hooks(\n",
    "        batch,\n",
    "        fwd_hooks=[\n",
    "            (training_cfg.hook_point, store_mlp_inputs),\n",
    "            (training_cfg.out_hook_point, store_mlp_output),\n",
    "        ],\n",
    "        return_type=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can unload gpu\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(mlp_inputs)=}, {mlp_inputs[0].shape=}\")\n",
    "print(f\"{len(mlp_outputs)=}, {mlp_outputs[0].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class MLPActivationsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mlp_inputs: list[Float[torch.Tensor, \"batch seq d_mlp_in\"]],\n",
    "        mlp_outputs: list[Float[torch.Tensor, \"batch seq d_mlp_out\"]],\n",
    "    ) -> None:\n",
    "        self.mlp_inputs = mlp_inputs\n",
    "        self.mlp_outputs = mlp_outputs\n",
    "        assert len(self.mlp_inputs) == len(\n",
    "            self.mlp_outputs\n",
    "        ), \"Inputs and outputs must be the same length.\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mlp_inputs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[\n",
    "        Float[torch.Tensor, \"batch seq d_mlp_in\"],\n",
    "        Float[torch.Tensor, \"batch seq d_mlp_out\"],\n",
    "    ]:\n",
    "        x = self.mlp_inputs[idx]  # Shape: [128, 128, 768]\n",
    "        y = self.mlp_outputs[idx]  # Shape: [128, 128, 768]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "activations_dataset = MLPActivationsDataset(mlp_inputs, mlp_outputs)\n",
    "activations_dataloader = DataLoader(\n",
    "    activations_dataset,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x.shape=torch.Size([1, 128, 128, 768]), batch_y.shape=torch.Size([1, 128, 128, 768])\n",
    "# for batch_x, batch_y in activations_dataloader:\n",
    "#     print(f\"{batch_x.shape=}, {batch_y.shape=}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0e565",
   "metadata": {},
   "source": [
    "# Train Transcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403769bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"transcoder_training_v2\",\n",
    "    config=dataclasses.asdict(training_cfg),\n",
    ")\n",
    "\n",
    "transcoder = Transcoder(cfg=transcoder_cfg)\n",
    "\n",
    "transcoder = transcoder.to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(transcoder.parameters(), lr=training_cfg.learning_rate)\n",
    "\n",
    "num_steps = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(training_cfg.num_epochs):\n",
    "\n",
    "    for batch_index, batch in tqdm.tqdm(\n",
    "        enumerate(activations_dataloader),\n",
    "        desc=f\"Epoch {epoch+1}/{training_cfg.num_epochs}\",\n",
    "    ):\n",
    "\n",
    "        # Do a training step.\n",
    "        transcoder.train()\n",
    "\n",
    "        # Make sure the W_dec is still zero-norm\n",
    "        transcoder.set_decoder_norm_to_unit_norm()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # move batch to device\n",
    "        batch_x, batch_y = batch\n",
    "\n",
    "        mlp_in = batch_x[0].to(device)\n",
    "        mlp_out = batch_y[0].to(device)\n",
    "\n",
    "        transcoder_results = transcoder(mlp_in)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_result = compute_loss(training_cfg, mlp_out, transcoder_results)\n",
    "\n",
    "        loss = loss_result.mse_loss + loss_result.l1_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        num_steps += 1\n",
    "\n",
    "        # Print loss statistics every 10 batches\n",
    "        if batch_index % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{training_cfg.num_epochs}, \"\n",
    "                f\"Batch {batch_index}/{len(activations_dataloader)}, \"\n",
    "                f\"Loss: {loss.item():.6f}, \"\n",
    "                f\"MSE Loss: {loss_result.mse_loss.item():.6f}, \"\n",
    "                f\"L1 Loss: {loss_result.l1_loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"mse_loss\": loss_result.mse_loss.item(),\n",
    "                    \"l1_loss\": loss_result.l1_loss.item(),\n",
    "                },\n",
    "                step=num_steps,\n",
    "            )\n",
    "\n",
    "    # Log model parameters and gradients\n",
    "    # wandb.watch(transcoder)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained transcoder model to a file\n",
    "import torch\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "model_save_path = \"full_transcoder_model.pth\"\n",
    "\n",
    "print(f\"Transcoder model saved to {model_save_path}\")\n",
    "\n",
    "torch.save(transcoder, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c030fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34937/1199069322.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_transcoder = torch.load(model_save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded transcoder\n",
      "Transcoder()\n",
      "Parameter: W_enc, Shape: torch.Size([768, 98304])\n",
      "Parameter: b_enc, Shape: torch.Size([98304])\n",
      "Parameter: W_dec, Shape: torch.Size([98304, 768])\n",
      "Parameter: b_dec, Shape: torch.Size([768])\n",
      "Parameter: b_dec_out, Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Define the path where you want to save the model\n",
    "model_save_path = \"full_transcoder_model.pth\"\n",
    "\n",
    "# Load the full transcoder model\n",
    "loaded_transcoder = torch.load(model_save_path)\n",
    "\n",
    "loaded_transcoder.to(device)\n",
    "\n",
    "print(\"Loaded transcoder\")\n",
    "\n",
    "# Set the loaded model to evaluation mode\n",
    "loaded_transcoder.eval()\n",
    "\n",
    "print(loaded_transcoder)  # Print the loaded model architecture\n",
    "\n",
    "# Optionally, you can verify the model's parameters\n",
    "for name, param in loaded_transcoder.named_parameters():\n",
    "    print(f\"Parameter: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074e379",
   "metadata": {},
   "source": [
    "# Compute Loss When Substituting MLP with Transcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d46b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TranscoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, transcoder: Transcoder):\n",
    "        super().__init__()\n",
    "        self.transcoder = transcoder\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[torch.Tensor, \"... d_in\"]\n",
    "    ) -> Float[torch.Tensor, \"... d_out\"]:\n",
    "        transcoder_result = self.transcoder(x)\n",
    "        return transcoder_result.transcoder_out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_test_loss_when_replacing_mlp_with_transcoder(\n",
    "    batch_tokens: Float[torch.Tensor, \"batch seq\"],\n",
    "    transcoder: Transcoder,\n",
    "    model: transformer_lens.HookedTransformer,\n",
    "    hook_point_layer: str,\n",
    ") -> Float[torch.Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    A method for running the model with the SAE activations in order to return the\n",
    "    loss returns per token loss when activations are substituted in.\n",
    "\n",
    "    \"\"\"\n",
    "    old_mlp = model.blocks[hook_point_layer]\n",
    "\n",
    "    model.blocks[hook_point_layer].mlp = _TranscoderWrapper(transcoder)\n",
    "\n",
    "    ce_loss_with_recons = model.run_with_hooks(batch_tokens, return_type=\"loss\")\n",
    "\n",
    "    model.blocks[hook_point_layer] = old_mlp\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return ce_loss_with_recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ef88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e87ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Tokenizing dataset...\n",
      "torch.Size([10, 128])\n",
      "Creating dataloader for dataset...\n",
      "Num batches: 0.078125\n",
      "avg_loss_original=0.3654099225997925\n",
      "avg_loss_when_replaced_mlp=0.36835944652557373\n"
     ]
    }
   ],
   "source": [
    "# compute how much worse this makes the loss\n",
    "#\n",
    "# note: normally compare to ablated\n",
    "#\n",
    "transcoder = loaded_transcoder\n",
    "\n",
    "transcoder.eval()\n",
    "\n",
    "num_batches = 10\n",
    "\n",
    "dataloader = create_tokenized_dataloader(num_samples=num_batches)\n",
    "\n",
    "avg_loss_original = 0\n",
    "avg_loss_when_replaced_mlp = 0\n",
    "\n",
    "for batch_index, batch in enumerate(dataloader):\n",
    "\n",
    "    if batch_index > num_batches:\n",
    "        break\n",
    "\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    loss_original = model.run_with_hooks(batch, return_type=\"loss\")\n",
    "\n",
    "    loss_when_replaced_mlp = get_test_loss_when_replacing_mlp_with_transcoder(\n",
    "        batch_tokens=batch,\n",
    "        transcoder=transcoder,\n",
    "        model=model,\n",
    "        hook_point_layer=training_cfg.hook_point_layer,\n",
    "    )\n",
    "\n",
    "    avg_loss_original += loss_original.item()\n",
    "    avg_loss_when_replaced_mlp += loss_when_replaced_mlp.item()\n",
    "\n",
    "avg_loss_original /= num_batches\n",
    "avg_loss_when_replaced_mlp /= num_batches\n",
    "\n",
    "print(f\"{avg_loss_original=}\")\n",
    "print(f\"{avg_loss_when_replaced_mlp=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501dc0af",
   "metadata": {},
   "source": [
    "# Sanity Check - Indirect Object Identification\n",
    "\n",
    "We quickly check that IOI isn't impacted (it shouldn't be, since we know it\n",
    "doesn't depend much on MLP, but it's good to check against a known result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "831791e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.24</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74.96</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.24\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m74.96\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.24 Prob: 74.96% Token: | Mary|\n",
      "Top 1th token. Logit: 15.73 Prob:  6.11% Token: | John|\n",
      "Top 2th token. Logit: 15.25 Prob:  3.75% Token: | the|\n",
      "Top 3th token. Logit: 14.83 Prob:  2.47% Token: | his|\n",
      "Top 4th token. Logit: 14.52 Prob:  1.81% Token: | them|\n",
      "Top 5th token. Logit: 13.58 Prob:  0.71% Token: | her|\n",
      "Top 6th token. Logit: 13.30 Prob:  0.53% Token: | a|\n",
      "Top 7th token. Logit: 13.01 Prob:  0.40% Token: | him|\n",
      "Top 8th token. Logit: 12.97 Prob:  0.39% Token: | their|\n",
      "Top 9th token. Logit: 12.90 Prob:  0.36% Token: | Jesus|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformer_lens\n",
    "\n",
    "# sanity check with an example\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    model,\n",
    "    prepend_bos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "291f262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.24</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74.96</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.24\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m74.96\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.24 Prob: 74.96% Token: | Mary|\n",
      "Top 1th token. Logit: 15.73 Prob:  6.11% Token: | John|\n",
      "Top 2th token. Logit: 15.25 Prob:  3.75% Token: | the|\n",
      "Top 3th token. Logit: 14.83 Prob:  2.47% Token: | his|\n",
      "Top 4th token. Logit: 14.52 Prob:  1.81% Token: | them|\n",
      "Top 5th token. Logit: 13.58 Prob:  0.71% Token: | her|\n",
      "Top 6th token. Logit: 13.30 Prob:  0.53% Token: | a|\n",
      "Top 7th token. Logit: 13.01 Prob:  0.40% Token: | him|\n",
      "Top 8th token. Logit: 12.97 Prob:  0.39% Token: | their|\n",
      "Top 9th token. Logit: 12.90 Prob:  0.36% Token: | Jesus|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "old_mlp = model.blocks[training_cfg.hook_point_layer]\n",
    "\n",
    "model.blocks[training_cfg.hook_point_layer].mlp = _TranscoderWrapper(transcoder)\n",
    "\n",
    "transformer_lens.utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    model,\n",
    "    prepend_bos=True,\n",
    ")\n",
    "\n",
    "model.blocks[training_cfg.hook_point_layer] = old_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ce066",
   "metadata": {},
   "source": [
    "# Differences In Generated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "699b86a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The speech is about\"\n",
    "\n",
    "generated_text = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0,\n",
    "    stop_at_eos=True,\n",
    ")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c2eb68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\" and the \"war on drugs.\"\n",
      "\n",
      "The speech is about the \"war on terror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The speech is about\"\n",
    "\n",
    "old_mlp = model.blocks[training_cfg.hook_point_layer]\n",
    "\n",
    "model.blocks[training_cfg.hook_point_layer].mlp = _TranscoderWrapper(transcoder)\n",
    "\n",
    "generated_text = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0,\n",
    "    stop_at_eos=True,\n",
    ")\n",
    "\n",
    "model.blocks[training_cfg.hook_point_layer] = old_mlp\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f91875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
