{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8136c449-8445-45fd-8d23-358f6e639e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some ipython magic to automatically reload any imports if they change\n",
    "# (useful when iterating locally)\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "# do this so that formatter not messed up\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1271761",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57a2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minTranscoder/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "# load the device we'll use (GPU or MPS)\n",
    "device = transformer_lens.utils.get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bb617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minTranscoder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model_name = \"gpt2-small\"\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1587a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.07</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m70.07\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.09 Prob: 70.07% Token: | Mary|\n",
      "Top 1th token. Logit: 15.38 Prob:  4.67% Token: | the|\n",
      "Top 2th token. Logit: 15.35 Prob:  4.54% Token: | John|\n",
      "Top 3th token. Logit: 15.25 Prob:  4.11% Token: | them|\n",
      "Top 4th token. Logit: 14.84 Prob:  2.73% Token: | his|\n",
      "Top 5th token. Logit: 14.06 Prob:  1.24% Token: | her|\n",
      "Top 6th token. Logit: 13.54 Prob:  0.74% Token: | a|\n",
      "Top 7th token. Logit: 13.52 Prob:  0.73% Token: | their|\n",
      "Top 8th token. Logit: 13.13 Prob:  0.49% Token: | Jesus|\n",
      "Top 9th token. Logit: 12.97 Prob:  0.42% Token: | him|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check with an example\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    model,\n",
    "    prepend_bos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54629d69",
   "metadata": {},
   "source": [
    "# Define Transcoder Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3c8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "\n",
    "from min_transcoder.transcoder import (\n",
    "    TranscoderResults,\n",
    "    TranscoderConfig,\n",
    "    Transcoder,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TranscoderTrainingConfig:\n",
    "\n",
    "    # Name of the layer to hook into for feature extraction\n",
    "    hook_point: str\n",
    "    out_hook_point: str\n",
    "\n",
    "    num_epochs: int = 100\n",
    "\n",
    "    # both from https://arxiv.org/html/2406.11944v1#S3 appendix E\n",
    "    learning_rate: float = 2 * 10e-5\n",
    "    l1_coefficient: float = 1e-4\n",
    "\n",
    "    @property\n",
    "    def hook_point_layer(self) -> int:\n",
    "        \"Parse out the hook point layer as int ex: 'blocks.8.ln2.hook_normalized' -> 8\"\n",
    "        return int(self.hook_point.split(\".\")[1])\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TranscoderLoss:\n",
    "    mse_loss: Float[torch.Tensor, \"\"]\n",
    "    l1_loss: Float[torch.Tensor, \"\"]\n",
    "\n",
    "\n",
    "def compute_loss(\n",
    "    cfg: TranscoderTrainingConfig,\n",
    "    mlp_out: Float[torch.Tensor, \"...\"],\n",
    "    results: TranscoderResults,\n",
    ") -> TranscoderLoss:\n",
    "\n",
    "    mse_loss_per_batch: Float[torch.Tensor, \"...\"] = (\n",
    "        torch.pow((results.transcoder_out - mlp_out.float()), 2)\n",
    "        / (mlp_out**2).sum(dim=-1, keepdim=True).sqrt()\n",
    "    )\n",
    "\n",
    "    mse_loss = mse_loss_per_batch.mean()\n",
    "\n",
    "    sparsity = torch.abs(results.hidden_activations).sum(dim=1).mean(dim=(0,))\n",
    "\n",
    "    # TODO(bschoen): Do we sum here?\n",
    "    l1_loss = cfg.l1_coefficient * sparsity.mean()\n",
    "\n",
    "    return TranscoderLoss(mse_loss=mse_loss, l1_loss=l1_loss)\n",
    "\n",
    "\n",
    "# from https://arxiv.org/html/2406.11944v1#S3 appendix E\n",
    "transcoder_expansion_factor = 32\n",
    "\n",
    "transcoder_cfg = TranscoderConfig(\n",
    "    d_in=model.cfg.d_model,\n",
    "    d_out=model.cfg.d_model,\n",
    "    # our transcoder has a hidden dimension of d_mlp * expansion factor\n",
    "    d_hidden=model.cfg.d_mlp * transcoder_expansion_factor,\n",
    "    dtype=model.cfg.dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8b12ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.cfg.n_layers=12\n",
      "model.cfg.d_mlp=3072\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model.cfg.n_layers=}\")\n",
    "print(f\"{model.cfg.d_mlp=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1cbe0b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26733ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def create_tokenized_dataloader(\n",
    "    max_length: int = 128,\n",
    "    batch_size: int = 64,\n",
    "    num_samples: int = 10000,\n",
    "    shuffle: bool = True,\n",
    ") -> DataLoader:\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = datasets.load_dataset(\n",
    "        path=\"NeelNanda/pile-10k\",\n",
    "        split=\"train\",\n",
    "        streaming=False,\n",
    "    )\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    token_dataset = transformer_lens.utils.tokenize_and_concatenate(\n",
    "        dataset=dataset,\n",
    "        tokenizer=model.tokenizer,\n",
    "        streaming=True,\n",
    "        max_length=max_length,\n",
    "        add_bos_token=model.cfg.default_prepend_bos,\n",
    "    )\n",
    "\n",
    "    # token_dataset['tokens'].shape=torch.Size([136625, 128])\n",
    "    # print(f\"{token_dataset['tokens'].shape=}\")\n",
    "\n",
    "    # shuffle, and arbitrarily cap at around 10,000 / 130,000 (original caps at ~24k)\n",
    "    token_dataset = token_dataset.shuffle(42)\n",
    "    token_dataset = token_dataset.take(num_samples)\n",
    "\n",
    "    token_dataset_torch = torch.from_numpy(\n",
    "        np.stack([x[\"tokens\"] for x in token_dataset])\n",
    "    ).cuda()\n",
    "\n",
    "    # torch.Size([100, 1024])\n",
    "    print(token_dataset_torch.shape)\n",
    "\n",
    "    # Create a DataLoader for batching\n",
    "    #\n",
    "    # for batch in dataloader:\n",
    "    #     print(batch.shape) # torch.Size([32, 1024])\n",
    "    #     break\n",
    "    #\n",
    "    print(f\"Creating dataloader for dataset...\")\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        token_dataset_torch,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "    print(f\"Num batches: {token_dataset_torch.shape[0] / batch_size}\")\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd288a",
   "metadata": {},
   "source": [
    "# Collect Activations\n",
    "\n",
    "Here we'll create hooks to store the MLP activations only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8c2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "layer = 11\n",
    "\n",
    "training_cfg = TranscoderTrainingConfig(\n",
    "    num_epochs=5,\n",
    "    hook_point=f\"blocks.{layer}.ln2.hook_normalized\",\n",
    "    out_hook_point=f\"blocks.{layer}.hook_mlp_out\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730a64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the MLP activations\n",
    "mlp_inputs: list[Float[torch.Tensor, \"batch seq d_mlp_in\"]] = []\n",
    "mlp_outputs: list[Float[torch.Tensor, \"batch seq d_mlp_out\"]] = []\n",
    "\n",
    "\n",
    "# TODO(bschoen): Could make this general\n",
    "def store_mlp_inputs(\n",
    "    mlp_input: Float[torch.Tensor, \"... d_in\"],\n",
    "    hook: transformer_lens.hook_points.HookPoint,\n",
    ") -> None:\n",
    "\n",
    "    # Detach and move to CPU to save memory\n",
    "    mlp_inputs.append(mlp_input.detach().cpu())\n",
    "\n",
    "\n",
    "def store_mlp_output(\n",
    "    mlp_output: Float[torch.Tensor, \"... d_out\"],\n",
    "    hook: transformer_lens.hook_points.HookPoint,\n",
    ") -> None:\n",
    "\n",
    "    # Detach and move to CPU to save memory\n",
    "    mlp_outputs.append(mlp_output.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eede8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Tokenizing dataset...\n",
      "torch.Size([10000, 128])\n",
      "Creating dataloader for dataset...\n",
      "Num batches: 156.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting MLP activations: 157it [00:24,  6.42it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_tokenized_dataloader()\n",
    "\n",
    "# put model itself into eval mode so doesn't change\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch in tqdm.tqdm(\n",
    "    enumerate(dataloader),\n",
    "    desc=\"Collecting MLP activations\",\n",
    "):\n",
    "    # move batch to device\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    # Get MLP input and output activations\n",
    "    model.run_with_hooks(\n",
    "        batch,\n",
    "        fwd_hooks=[\n",
    "            (training_cfg.hook_point, store_mlp_inputs),\n",
    "            (training_cfg.out_hook_point, store_mlp_output),\n",
    "        ],\n",
    "        return_type=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a085bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can unload gpu\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aaf7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(mlp_inputs)=157, mlp_inputs[0].shape=torch.Size([64, 128, 768])\n",
      "len(mlp_outputs)=157, mlp_outputs[0].shape=torch.Size([64, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(mlp_inputs)=}, {mlp_inputs[0].shape=}\")\n",
    "print(f\"{len(mlp_outputs)=}, {mlp_outputs[0].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d7b96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class MLPActivationsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mlp_inputs: list[Float[torch.Tensor, \"batch seq d_mlp_in\"]],\n",
    "        mlp_outputs: list[Float[torch.Tensor, \"batch seq d_mlp_out\"]],\n",
    "    ) -> None:\n",
    "        self.mlp_inputs = mlp_inputs\n",
    "        self.mlp_outputs = mlp_outputs\n",
    "        assert len(self.mlp_inputs) == len(\n",
    "            self.mlp_outputs\n",
    "        ), \"Inputs and outputs must be the same length.\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mlp_inputs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[\n",
    "        Float[torch.Tensor, \"batch seq d_mlp_in\"],\n",
    "        Float[torch.Tensor, \"batch seq d_mlp_out\"],\n",
    "    ]:\n",
    "        x = self.mlp_inputs[idx]  # Shape: [128, 128, 768]\n",
    "        y = self.mlp_outputs[idx]  # Shape: [128, 128, 768]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "activations_dataset = MLPActivationsDataset(mlp_inputs, mlp_outputs)\n",
    "activations_dataloader = DataLoader(\n",
    "    activations_dataset,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0e565",
   "metadata": {},
   "source": [
    "# Train Transcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "403769bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbronsonschoen\u001b[0m (\u001b[33mbronsonschoen-personal-use\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/minTranscoder/wandb/run-20240915_161952-8bipu47h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2/runs/8bipu47h' target=\"_blank\">zany-puddle-8</a></strong> to <a href='https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2/runs/8bipu47h' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2/runs/8bipu47h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 1it [00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 0/79, Loss: 0.112818, MSE Loss: 0.112178, L1 Loss: 0.000640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 11it [00:10,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 10/79, Loss: 0.038793, MSE Loss: 0.038266, L1 Loss: 0.000527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 21it [00:19,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 20/79, Loss: 0.028430, MSE Loss: 0.027941, L1 Loss: 0.000489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 31it [00:27,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 30/79, Loss: 0.025092, MSE Loss: 0.024633, L1 Loss: 0.000460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 41it [00:36,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 40/79, Loss: 0.023120, MSE Loss: 0.022663, L1 Loss: 0.000457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 51it [00:45,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 50/79, Loss: 0.021735, MSE Loss: 0.021267, L1 Loss: 0.000469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 61it [00:54,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 60/79, Loss: 0.021578, MSE Loss: 0.021100, L1 Loss: 0.000478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 71it [01:03,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 70/79, Loss: 0.019396, MSE Loss: 0.018922, L1 Loss: 0.000474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 79it [01:10,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>l1_loss</td><td>█▄▂▁▁▁▂▂</td></tr><tr><td>loss</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>mse_loss</td><td>█▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>l1_loss</td><td>0.00047</td></tr><tr><td>loss</td><td>0.0194</td></tr><tr><td>mse_loss</td><td>0.01892</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-puddle-8</strong> at: <a href='https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2/runs/8bipu47h' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2/runs/8bipu47h</a><br/> View project at: <a href='https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/transcoder_training_v2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240915_161952-8bipu47h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"transcoder_training_v2\",\n",
    "    config=dataclasses.asdict(training_cfg),\n",
    ")\n",
    "\n",
    "transcoder = Transcoder(cfg=transcoder_cfg)\n",
    "\n",
    "transcoder = transcoder.to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(transcoder.parameters(), lr=training_cfg.learning_rate)\n",
    "\n",
    "num_steps = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(training_cfg.num_epochs):\n",
    "\n",
    "    for batch_index, batch in tqdm.tqdm(\n",
    "        enumerate(activations_dataloader),\n",
    "        desc=f\"Epoch {epoch+1}/{training_cfg.num_epochs}\",\n",
    "    ):\n",
    "\n",
    "        # Do a training step.\n",
    "        transcoder.train()\n",
    "\n",
    "        # Make sure the W_dec is still zero-norm\n",
    "        transcoder.set_decoder_norm_to_unit_norm()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # move batch to device\n",
    "        batch_x, batch_y = batch\n",
    "\n",
    "        mlp_in = batch_x[0].to(device)\n",
    "        mlp_out = batch_y[0].to(device)\n",
    "\n",
    "        transcoder_results = transcoder(mlp_in)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_result = compute_loss(training_cfg, mlp_out, transcoder_results)\n",
    "\n",
    "        loss = loss_result.mse_loss + loss_result.l1_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        num_steps += 1\n",
    "\n",
    "        # Print loss statistics every 10 batches\n",
    "        if batch_index % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{training_cfg.num_epochs}, \"\n",
    "                f\"Batch {batch_index}/{len(activations_dataloader)}, \"\n",
    "                f\"Loss: {loss.item():.6f}, \"\n",
    "                f\"MSE Loss: {loss_result.mse_loss.item():.6f}, \"\n",
    "                f\"L1 Loss: {loss_result.l1_loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"mse_loss\": loss_result.mse_loss.item(),\n",
    "                    \"l1_loss\": loss_result.l1_loss.item(),\n",
    "                },\n",
    "                step=num_steps,\n",
    "            )\n",
    "\n",
    "    # Log model parameters and gradients\n",
    "    # wandb.watch(transcoder)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4bf8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcoder model saved to full_transcoder_model_blocks.11.ln2.hook_normalized.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained transcoder model to a file\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "model_save_path = f\"full_transcoder_model_{training_cfg.hook_point}.pth\"\n",
    "\n",
    "print(f\"Transcoder model saved to {model_save_path}\")\n",
    "\n",
    "if not pathlib.Path(model_save_path).exists():\n",
    "    torch.save(transcoder, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c030fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45132/3283311484.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_transcoder = torch.load(model_save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded transcoder\n",
      "Transcoder()\n",
      "Parameter: W_enc, Shape: torch.Size([768, 98304])\n",
      "Parameter: b_enc, Shape: torch.Size([98304])\n",
      "Parameter: W_dec, Shape: torch.Size([98304, 768])\n",
      "Parameter: b_dec, Shape: torch.Size([768])\n",
      "Parameter: b_dec_out, Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Load the full transcoder model\n",
    "loaded_transcoder = torch.load(model_save_path)\n",
    "\n",
    "loaded_transcoder.to(device)\n",
    "\n",
    "print(\"Loaded transcoder\")\n",
    "\n",
    "# Set the loaded model to evaluation mode\n",
    "loaded_transcoder.eval()\n",
    "\n",
    "print(loaded_transcoder)  # Print the loaded model architecture\n",
    "\n",
    "# Optionally, you can verify the model's parameters\n",
    "for name, param in loaded_transcoder.named_parameters():\n",
    "    print(f\"Parameter: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074e379",
   "metadata": {},
   "source": [
    "# Compute Loss When Substituting MLP with Transcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d46b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TranscoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, transcoder: Transcoder):\n",
    "        super().__init__()\n",
    "        self.transcoder = transcoder\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[torch.Tensor, \"... d_in\"]\n",
    "    ) -> Float[torch.Tensor, \"... d_out\"]:\n",
    "        transcoder_result = self.transcoder(x)\n",
    "        return transcoder_result.transcoder_out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_test_loss_when_replacing_mlp_with_transcoder(\n",
    "    batch_tokens: Float[torch.Tensor, \"batch seq\"],\n",
    "    transcoder: Transcoder,\n",
    "    model: transformer_lens.HookedTransformer,\n",
    "    hook_point_layer: str,\n",
    ") -> Float[torch.Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    A method for running the model with the SAE activations in order to return the\n",
    "    loss returns per token loss when activations are substituted in.\n",
    "\n",
    "    \"\"\"\n",
    "    old_mlp = model.blocks[hook_point_layer]\n",
    "\n",
    "    model.blocks[hook_point_layer].mlp = _TranscoderWrapper(transcoder)\n",
    "\n",
    "    ce_loss_with_recons = model.run_with_hooks(batch_tokens, return_type=\"loss\")\n",
    "\n",
    "    model.blocks[hook_point_layer] = old_mlp\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return ce_loss_with_recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ef88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e87ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Tokenizing dataset...\n",
      "torch.Size([10, 128])\n",
      "Creating dataloader for dataset...\n",
      "Num batches: 0.15625\n",
      "avg_loss_original=0.3654099225997925\n",
      "avg_loss_when_replaced_mlp=0.37158894538879395\n"
     ]
    }
   ],
   "source": [
    "# compute how much worse this makes the loss\n",
    "#\n",
    "# note: normally compare to ablated\n",
    "#\n",
    "transcoder = loaded_transcoder\n",
    "\n",
    "transcoder.eval()\n",
    "\n",
    "num_batches = 10\n",
    "\n",
    "dataloader = create_tokenized_dataloader(num_samples=num_batches)\n",
    "\n",
    "avg_loss_original = 0\n",
    "avg_loss_when_replaced_mlp = 0\n",
    "\n",
    "for batch_index, batch in enumerate(dataloader):\n",
    "\n",
    "    if batch_index > num_batches:\n",
    "        break\n",
    "\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    loss_original = model.run_with_hooks(batch, return_type=\"loss\")\n",
    "\n",
    "    loss_when_replaced_mlp = get_test_loss_when_replacing_mlp_with_transcoder(\n",
    "        batch_tokens=batch,\n",
    "        transcoder=transcoder,\n",
    "        model=model,\n",
    "        hook_point_layer=training_cfg.hook_point_layer,\n",
    "    )\n",
    "\n",
    "    avg_loss_original += loss_original.item()\n",
    "    avg_loss_when_replaced_mlp += loss_when_replaced_mlp.item()\n",
    "\n",
    "avg_loss_original /= num_batches\n",
    "avg_loss_when_replaced_mlp /= num_batches\n",
    "\n",
    "print(f\"{avg_loss_original=}\")\n",
    "print(f\"{avg_loss_when_replaced_mlp=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501dc0af",
   "metadata": {},
   "source": [
    "# Sanity Check - Indirect Object Identification\n",
    "\n",
    "We quickly check that IOI isn't impacted (it shouldn't be, since we know it\n",
    "doesn't depend much on MLP, but it's good to check against a known result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "831791e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.39</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75.11</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.39\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m75.11\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.39 Prob: 75.11% Token: | Mary|\n",
      "Top 1th token. Logit: 15.74 Prob:  5.33% Token: | John|\n",
      "Top 2th token. Logit: 15.69 Prob:  5.03% Token: | the|\n",
      "Top 3th token. Logit: 14.92 Prob:  2.35% Token: | them|\n",
      "Top 4th token. Logit: 14.24 Prob:  1.19% Token: | his|\n",
      "Top 5th token. Logit: 13.88 Prob:  0.83% Token: | a|\n",
      "Top 6th token. Logit: 13.34 Prob:  0.48% Token: | her|\n",
      "Top 7th token. Logit: 13.26 Prob:  0.45% Token: | their|\n",
      "Top 8th token. Logit: 13.25 Prob:  0.44% Token: | Jesus|\n",
      "Top 9th token. Logit: 13.17 Prob:  0.41% Token: | Mrs|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformer_lens\n",
    "\n",
    "# sanity check with an example\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    model,\n",
    "    prepend_bos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "291f262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.39</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75.11</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.39\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m75.11\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.39 Prob: 75.11% Token: | Mary|\n",
      "Top 1th token. Logit: 15.74 Prob:  5.33% Token: | John|\n",
      "Top 2th token. Logit: 15.69 Prob:  5.03% Token: | the|\n",
      "Top 3th token. Logit: 14.92 Prob:  2.35% Token: | them|\n",
      "Top 4th token. Logit: 14.24 Prob:  1.19% Token: | his|\n",
      "Top 5th token. Logit: 13.88 Prob:  0.83% Token: | a|\n",
      "Top 6th token. Logit: 13.34 Prob:  0.48% Token: | her|\n",
      "Top 7th token. Logit: 13.26 Prob:  0.45% Token: | their|\n",
      "Top 8th token. Logit: 13.25 Prob:  0.44% Token: | Jesus|\n",
      "Top 9th token. Logit: 13.17 Prob:  0.41% Token: | Mrs|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "old_mlp = model.blocks[training_cfg.hook_point_layer]\n",
    "\n",
    "model.blocks[training_cfg.hook_point_layer].mlp = _TranscoderWrapper(transcoder)\n",
    "\n",
    "transformer_lens.utils.test_prompt(\n",
    "    example_prompt,\n",
    "    example_answer,\n",
    "    model,\n",
    "    prepend_bos=True,\n",
    ")\n",
    "\n",
    "model.blocks[training_cfg.hook_point_layer] = old_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ce066",
   "metadata": {},
   "source": [
    "# Differences In Generated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699b86a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 43.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speech is about the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The speech is about\"\n",
    "\n",
    "generated_text = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0,\n",
    "    stop_at_eos=True,\n",
    ")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c2eb68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speech is about the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"new\" American culture of \"self-expression,\" which is the \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The speech is about\"\n",
    "\n",
    "old_mlp = model.blocks[training_cfg.hook_point_layer]\n",
    "\n",
    "model.blocks[training_cfg.hook_point_layer].mlp = _TranscoderWrapper(transcoder)\n",
    "\n",
    "generated_text = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0,\n",
    "    stop_at_eos=True,\n",
    ")\n",
    "\n",
    "model.blocks[training_cfg.hook_point_layer] = old_mlp\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f91875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20862c61",
   "metadata": {},
   "source": [
    "# Top Activating Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d44f9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can unload gpu\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1624bdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Tokenizing dataset...\n",
      "torch.Size([10000, 128])\n",
      "Creating dataloader for dataset...\n",
      "Num batches: 156.25\n"
     ]
    }
   ],
   "source": [
    "# don't shuffle, that way we can lookup token batches by index\n",
    "dataloader = create_tokenized_dataloader(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39a77db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = len(dataloader.batch_sampler)\n",
    "\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c8006c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting MLP activations: 157it [01:11,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# store the MLP activations\n",
    "mlp_inputs: list[Float[torch.Tensor, \"batch seq d_mlp_in\"]] = []\n",
    "mlp_outputs: list[Float[torch.Tensor, \"batch seq d_mlp_out\"]] = []\n",
    "reconstructed_mlp_outputs: list[Float[torch.Tensor, \"batch seq d_mlp_out\"]] = []\n",
    "\n",
    "# put model itself into eval mode so doesn't change\n",
    "model.eval()\n",
    "transcoder.eval()\n",
    "\n",
    "for batch_index, batch in tqdm.tqdm(\n",
    "    enumerate(dataloader),\n",
    "    desc=\"Collecting MLP activations\",\n",
    "):\n",
    "    # move batch to device\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    # Get MLP input and output activations\n",
    "    model.run_with_hooks(\n",
    "        batch,\n",
    "        fwd_hooks=[\n",
    "            (training_cfg.hook_point, store_mlp_inputs),\n",
    "            (training_cfg.out_hook_point, store_mlp_output),\n",
    "        ],\n",
    "        return_type=None,\n",
    "    )\n",
    "\n",
    "    # also reconstruct the mlp outputs using the transcoder\n",
    "    mlp_input = mlp_inputs[-1].to(device)\n",
    "\n",
    "    transcoder_result = transcoder(mlp_input)\n",
    "\n",
    "    reconstructed_mlp_output = transcoder_result.transcoder_out\n",
    "\n",
    "    reconstructed_mlp_outputs.append(reconstructed_mlp_output.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49b60dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "# access corresponding batch directly\n",
    "batch = dataloader.dataset[batch_index : batch_index + dataloader.batch_size]\n",
    "\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1ce6dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "009b1823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_inputs_tensor.shape=torch.Size([156, 64, 128, 768])\n",
      "mlp_outputs_tensor.shape=torch.Size([156, 64, 128, 768])\n",
      "reconstructed_mlp_outputs_tensor.shape=torch.Size([156, 64, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "def stack_list_of_tensors(\n",
    "    tensors: list[Float[torch.Tensor, \"...\"]]\n",
    ") -> Float[torch.Tensor, \"num_batches ...\"]:\n",
    "\n",
    "    # Drop the last batch in case it doesn't match the rest of that shapes (ex: size 16 when batch size 128)\n",
    "    if tensors[-1].shape[0] != tensors[0].shape[0]:\n",
    "        tensors = tensors[:-1]\n",
    "\n",
    "    return torch.stack(tensors)\n",
    "\n",
    "\n",
    "# Now we can safely stack the tensors\n",
    "mlp_inputs_tensor: Float[torch.Tensor, \"num_batches batch seq d_model\"] = (\n",
    "    stack_list_of_tensors(mlp_inputs)\n",
    ")\n",
    "mlp_outputs_tensor: Float[torch.Tensor, \"num_batches batch seq d_model\"] = (\n",
    "    stack_list_of_tensors(mlp_outputs)\n",
    ")\n",
    "reconstructed_mlp_outputs_tensor: Float[\n",
    "    torch.Tensor, \"num_batches batch seq d_model\"\n",
    "] = stack_list_of_tensors(reconstructed_mlp_outputs)\n",
    "\n",
    "print(f\"{mlp_inputs_tensor.shape=}\")\n",
    "print(f\"{mlp_outputs_tensor.shape=}\")\n",
    "print(f\"{reconstructed_mlp_outputs_tensor.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d5ce12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_inputs_tensor.shape=torch.Size([156, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "# reshape out input dataset to match these as well\n",
    "#\n",
    "# 9984 = num_batches=156 * batch=64 (original num samples is 10000)\n",
    "num_batches = mlp_outputs_tensor.shape[0]\n",
    "batch_size = mlp_outputs_tensor.shape[1]\n",
    "num_samples = num_batches * batch_size  # Use batch_size instead of batch\n",
    "\n",
    "# dataloader.dataset is `torch.Size([10000, 128])`\n",
    "# Remove the second dimension in slicing\n",
    "token_inputs = dataloader.dataset[:num_samples]\n",
    "\n",
    "token_inputs_tensor = einops.rearrange(\n",
    "    token_inputs,\n",
    "    \"(num_batches batch) seq -> num_batches batch seq\",\n",
    "    num_batches=num_batches,\n",
    ")\n",
    "\n",
    "print(f\"{token_inputs_tensor.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae8aeaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([156, 64, 128])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_inputs_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed6071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
